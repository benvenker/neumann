{"id": "nm-075", "content_hash": "nm-075", "title": "search --json prints banner", "description": "\\nProblem: running 'neumann search --json' prints 'Using ChromaDB at ...' before the JSON payload, so downstream jq/automation fails.\\nFix: suppress or redirect banner text whenever --json is set (or write diagnostics to stderr).\\nAcceptance: search --json emits pure JSON, no leading/trailing noise; human-readable banner still shown for normal runs.\\n", "status": "closed", "priority": 2, "issue_type": "bug", "created_at": "2025-11-14T21:30:28.02414-06:00", "updated_at": "2025-11-14T21:33:54.982166-06:00", "closed_at": "2025-11-14T21:33:54.982166-06:00", "source_repo": "."}
{"id": "nm-1", "content_hash": "nm-1", "title": "Implement Code Review Changes", "description": "Implement all recommendations from the GPT-5 Pro code review to align the codebase with a pages-first philosophy and fix quality issues. This includes changing defaults from \"both pages and tiles\" to \"pages only\", fixing resource leaks with context managers, improving grid tiler edge coverage, tightening CSS styling, updating tests, and lowering Python version requirement to 3.10 for better AI package compatibility.\n\nKey Changes:\n- Make pages-only the default (emit=pages, manifest=none)\n- Fix resource leaks in PDF and image handling\n- Improve grid tiler to guarantee edge coverage\n- Tighten line-number gutter CSS styling\n- Update all documentation to reflect new defaults\n- Add comprehensive tests for default behavior\n- Lower Python requirement from 3.13 to 3.10", "status": "closed", "priority": 2, "issue_type": "epic", "assignee": "ProductThor", "created_at": "2025-10-29T20:39:25.299684-05:00", "updated_at": "2025-10-29T21:17:40.958777-05:00", "closed_at": "2025-10-29T21:17:40.958777-05:00", "source_repo": "."}
{"id": "nm-10", "content_hash": "nm-10", "title": "Fix Grid Tiler Edge Coverage", "description": "Replace the current grid tiler implementation to guarantee complete edge coverage. The current implementation risks missing content at the right and bottom edges of images. The new implementation must generate coordinate lists that ensure the final tile covers the rightmost and bottommost edges, with proper clamping to prevent out-of-bounds access.\n\nImplementation Details:\n- Replace `tile_grid()` function (lines 266-281) with improved version\n- Generate x/y coordinate lists ensuring right/bottom edges are covered\n- Handle edge case where last tile would extend beyond image bounds\n- Use `min()` to clamp crops and prevent out-of-bounds access\n- Ensure all image content is included in tiles, even near edges", "status": "closed", "priority": 2, "issue_type": "task", "assignee": "ProductThor", "created_at": "2025-10-29T21:10:46.961075-05:00", "updated_at": "2025-11-01T11:30:46.646851-05:00", "closed_at": "2025-11-01T11:30:46.646851-05:00", "source_repo": "."}
{"id": "nm-11", "content_hash": "nm-11", "title": "Update Documentation", "description": "Update both README.md and docs/IMPLEMENTATION_PLAN.md to accurately reflect the new pages-only default behavior. Update examples, option descriptions, output structure documentation, and design decision rationale to align with the pages-first philosophy.\n\nImplementation Details:\n- **README.md**:\n  - Update description to emphasize pages-first, tiles optional (line 3)\n  - Update \"Basic Example\" section to show pages-only default (lines 54-60)\n  - Add separate section for tiles showing it as optional (lines 62-78)\n  - Update `--emit` and `--manifest` defaults in options list (lines 95-96)\n  - Update output structure section to show pages-only default (lines 99-114)\n  - Add note about line-number styling\n- **docs/IMPLEMENTATION_PLAN.md**:\n  - Update \"Default Behavior\" section (lines 9-12): pages-only, manifest=none\n  - Update CLI examples to reflect new defaults (lines 63-77, 99-100)\n  - Update \"Design Decisions\" section to explain pages-first rationale (lines 179-183)", "status": "closed", "priority": 2, "issue_type": "task", "assignee": "ProductThor", "created_at": "2025-10-29T21:16:47.039499-05:00", "updated_at": "2025-11-01T11:30:52.004037-05:00", "closed_at": "2025-11-01T11:30:52.004037-05:00", "source_repo": "."}
{"id": "nm-12", "content_hash": "nm-12", "title": "Create Configuration Module", "description": "Create config.py with Pydantic models for type-safe configuration management. Load settings from environment variables with sensible defaults.\n\nImplementation Details:\n- Use Pydantic BaseSettings for automatic env loading\n- Fields: ASSET_BASE_URL, CHROMA_PATH, OPENAI_API_KEY, LINES_PER_CHUNK, OVERLAP\n- Validation: ensure URLs are well-formed, paths exist, API key is present\n- Export a singleton `config` instance for import by other modules\n\nTesting:\n- Unit test: test_config_loads_defaults\n- Unit test: test_config_loads_from_env\n- Unit test: test_config_validation_fails_on_missing_api_key\n\nAcceptance:\n- config.py imports cleanly\n- All settings accessible via `from config import config`\n- .env.example provided with sample values\n- Type hints pass mypy checks", "notes": "Labels: component:config, phase:foundation, tech:pydantic", "status": "closed", "priority": 1, "issue_type": "task", "created_at": "2025-10-29T22:45:15.775714-05:00", "updated_at": "2025-10-30T17:30:18.145141-05:00", "closed_at": "2025-10-30T17:30:18.145141-05:00", "source_repo": ".", "labels": ["component:config", "phase:foundation", "tech:pydantic"]}
{"id": "nm-13", "content_hash": "nm-13", "title": "Add Dependencies to pyproject.toml", "description": "Add required dependencies for MVP: chromadb, openai, pyyaml, python-dotenv, pydantic-settings. Update pyproject.toml and lock file.\n\nDependencies to Add:\n- chromadb>=0.4.0\n- openai>=1.0.0\n- pyyaml>=6.0\n- python-dotenv>=1.0.0\n- pydantic>=2.0\n- pydantic-settings>=2.0\n\nTesting:\n- Verify all packages install: `uv pip install -e \".[dev]\"`\n- Verify imports work: `python -c \"import chromadb; import openai\"`\n\nAcceptance:\n- All dependencies in pyproject.toml\n- uv.lock updated\n- Dependencies install without conflicts\n- Import smoke test passes", "notes": "Labels: component:config, phase:foundation", "status": "closed", "priority": 1, "issue_type": "task", "assignee": "ProductThor", "created_at": "2025-10-29T22:45:20.668563-05:00", "updated_at": "2025-10-29T23:40:05.372476-05:00", "closed_at": "2025-10-29T23:40:05.372476-05:00", "source_repo": ".", "labels": ["component:config", "phase:foundation"]}
{"id": "nm-14", "content_hash": "nm-14", "title": "Add URI Generation to Renderer", "description": "Modify render_to_webp.py to generate HTTP URIs for each page WebP using the ASSET_BASE_URL from config. Update the manifest generation to include the `uri` field.\n\nImplementation Details:\n- Import `config` from config module\n- In pdf_to_webp_pages(), build URI for each page:\n  - Format: {ASSET_BASE_URL}/out/{doc_id}/pages/{filename}\n  - Example: http://127.0.0.1:8000/out/src__auth__module.ts/pages/src__auth__module-p001.webp\n- Pass URIs through to manifest generation\n- Update manifest dict to include `uri` field\n\nReference: main-spec.md \u00a73.1 (Pages manifest example)\n\nTesting:\n- Unit test: test_uri_generation_format\n- Integration test: render a file and verify URI in pages.jsonl\n- Manual test: verify URI is accessible via http.server\n\nAcceptance:\n- pages.jsonl contains `uri` field for every page\n- URIs are well-formed and resolvable\n- Backward compatible with existing manifest fields", "notes": "Labels: component:rendering, phase:foundation", "status": "closed", "priority": 1, "issue_type": "feature", "assignee": "ProductThor", "created_at": "2025-10-29T22:45:28.3968-05:00", "updated_at": "2025-10-30T09:12:43.053473-05:00", "closed_at": "2025-10-30T09:12:43.053473-05:00", "source_repo": ".", "labels": ["component:rendering", "phase:foundation"], "dependencies": [{"issue_id": "nm-14", "depends_on_id": "nm-12", "type": "blocks", "created_at": "2025-10-29T22:47:16.126337-05:00", "created_by": "daemon"}]}
{"id": "nm-15", "content_hash": "nm-15", "title": "Add Image Dimensions and File Size to Manifest", "description": "Extract WebP image dimensions (width, height) and file size (bytes) for each page. Add these fields to the pages.jsonl manifest.\n\nImplementation Details:\n- In pdf_to_webp_pages(), after saving WebP:\n  - Get file size: `out_path.stat().st_size`\n  - Already have dimensions from PIL Image: `img.width`, `img.height`\n- Return tuple: (out_path, width, height, bytes)\n- Update manifest dict to include: `width`, `height`, `bytes`\n\nReference: main-spec.md \u00a71.1 (pages.jsonl fields), \u00a72.2 (Data artifacts)\n\nTesting:\n- Unit test: test_dimensions_match_actual_image\n- Unit test: test_file_size_is_accurate\n- Integration test: render and verify all fields present\n\nAcceptance:\n- pages.jsonl includes `bytes`, `width`, `height` for every page\n- Values are accurate (verified against actual files)\n- No breaking changes to existing fields", "notes": "Labels: component:rendering, phase:foundation", "status": "closed", "priority": 1, "issue_type": "feature", "assignee": "ProductThor", "created_at": "2025-10-29T22:45:46.148614-05:00", "updated_at": "2025-10-30T09:12:48.400058-05:00", "closed_at": "2025-10-30T09:12:48.400058-05:00", "source_repo": ".", "labels": ["component:rendering", "phase:foundation"], "dependencies": [{"issue_id": "nm-15", "depends_on_id": "nm-14", "type": "blocks", "created_at": "2025-10-29T22:47:18.581391-05:00", "created_by": "daemon"}]}
{"id": "nm-16", "content_hash": "nm-16", "title": "Emit pages.jsonl in Pages Directory", "description": "Modify render_file() to emit a pages.jsonl manifest file in the pages directory (not tiles directory) with all required fields per the spec.\n\nImplementation Details:\n- In render_file(), after pdf_to_webp_pages() call:\n  - Build list of page records with all fields: doc_id, page, uri, sha256, bytes, width, height, source_pdf, source_file\n  - Write to `pages_dir / \"pages.jsonl\"`\n  - One JSON object per line (JSONL format)\n- Keep existing pages.txt for backward compatibility\n\nReference: main-spec.md \u00a72.2 (pages.jsonl structure)\n\nTesting:\n- Integration test: render multiple files, verify pages.jsonl exists\n- Unit test: test_jsonl_format_is_valid\n- Unit test: test_all_required_fields_present\n\nAcceptance:\n- pages.jsonl written to <out_dir>/<doc_id>/pages/pages.jsonl\n- Valid JSONL format (one object per line)\n- All fields match spec: doc_id, page, uri, sha256, bytes, width, height, source_pdf, source_file\n- Can be parsed line-by-line", "notes": "Labels: component:rendering, phase:foundation", "status": "closed", "priority": 1, "issue_type": "task", "assignee": "ProductThor", "created_at": "2025-10-29T22:45:56.864943-05:00", "updated_at": "2025-10-30T00:44:48.661529-05:00", "closed_at": "2025-10-30T00:44:48.661529-05:00", "source_repo": ".", "labels": ["component:rendering", "phase:foundation"], "dependencies": [{"issue_id": "nm-16", "depends_on_id": "nm-15", "type": "blocks", "created_at": "2025-10-29T22:47:21.307095-05:00", "created_by": "daemon"}]}
{"id": "nm-17", "content_hash": "nm-17", "title": "Create Embeddings Module", "description": "Create embeddings.py module with embed_texts() function using OpenAI's text-embedding-3-small model. Support batch processing for efficiency.\n\nImplementation Details:\n- Use OpenAI SDK with API key from config\n- Function signature: `embed_texts(texts: list[str], model: str = \"text-embedding-3-small\") -> list[list[float]]`\n- Batch up to 2048 texts per API call (OpenAI limit)\n- Return 1536-dimensional vectors\n- Handle rate limiting with exponential backoff\n- Add error handling for network issues\n\nReference: main-spec.md \u00a73.3 (Embeddings signature), \u00a72.6 (OpenAI embeddings)\n\nTesting:\n- Unit test: test_embed_single_text\n- Unit test: test_embed_batch (10 texts)\n- Unit test: test_vector_dimension_is_1536\n- Mock test: test_handles_api_errors\n- Integration test: test_with_real_api (optional, requires API key)\n\nAcceptance:\n- embed_texts() returns correct number of vectors\n- Each vector is 1536-dimensional\n- Batching works for large inputs\n- Errors are handled gracefully", "notes": "Labels: component:summarization, tech:openai, phase:core", "status": "closed", "priority": 1, "issue_type": "feature", "created_at": "2025-10-29T22:45:57.037035-05:00", "updated_at": "2025-10-30T17:32:29.186305-05:00", "closed_at": "2025-10-30T17:32:29.186305-05:00", "source_repo": ".", "labels": ["component:summarization", "phase:core", "tech:openai"], "dependencies": [{"issue_id": "nm-17", "depends_on_id": "nm-12", "type": "blocks", "created_at": "2025-10-29T22:47:24.214985-05:00", "created_by": "daemon"}, {"issue_id": "nm-17", "depends_on_id": "nm-13", "type": "blocks", "created_at": "2025-10-29T22:47:25.05494-05:00", "created_by": "daemon"}]}
{"id": "nm-18", "content_hash": "nm-18", "title": "Create Pydantic Models for Summary Schema", "description": "Create Pydantic models for the summary YAML front-matter and full summary structure. This ensures type safety and validation for LLM-generated summaries.\n\nImplementation Details:\n- Create models.py with Pydantic models\n- SummaryFrontMatter: doc_id, source_path, language, product_tags, last_updated, key_topics, api_symbols, related_files, suggested_queries\n- FileSummary: front_matter (SummaryFrontMatter), summary_md (str)\n- Add validators for field constraints (e.g., summary_md length 200-400 words)\n- Provide .to_yaml() method for serialization\n\nReference: main-spec.md \u00a72.2 (Summary files structure), \u00a73.1 (YAML example)\n\nTesting:\n- Unit test: test_valid_summary_passes_validation\n- Unit test: test_short_summary_fails_validation\n- Unit test: test_to_yaml_serialization\n\nAcceptance:\n- Models validate all required fields\n- Word count validation works\n- YAML serialization produces correct format\n- Type hints are complete", "notes": "Labels: component:summarization, tech:pydantic, phase:core", "status": "closed", "priority": 1, "issue_type": "task", "assignee": "ProductThor", "created_at": "2025-10-29T22:45:57.201234-05:00", "updated_at": "2025-10-29T23:44:36.97656-05:00", "closed_at": "2025-10-29T23:44:36.97656-05:00", "source_repo": ".", "labels": ["component:summarization", "phase:core", "tech:pydantic"], "dependencies": [{"issue_id": "nm-18", "depends_on_id": "nm-13", "type": "blocks", "created_at": "2025-10-29T22:47:27.17912-05:00", "created_by": "daemon"}]}
{"id": "nm-19", "content_hash": "nm-19", "title": "Implement Summarization with OpenAI Structured Output", "description": "Create summarize.py with summarize_file() function that uses OpenAI's 4o-mini with structured output to generate summaries matching the Pydantic schema.\n\nImplementation Details:\n- Use OpenAI SDK with structured output (response_format parameter)\n- Prompt engineering for retrieval-oriented summaries\n- Function signature: `summarize_file(source_path: str, text: str) -> FileSummary`\n- Generate doc_id from path\n- Auto-detect language from file extension\n- Return validated Pydantic model\n- Save as .summary.md with YAML front-matter\n\nSystem Prompt: Focus on retrieval-oriented summaries with purpose, key functionality, patterns, and codebase context.\n\nReference: main-spec.md \u00a71.2 (Summaries), \u00a72.4 (Summarizer signature)\n\nTesting:\n- Unit test: test_summarize_generates_valid_summary (mocked LLM)\n- Unit test: test_doc_id_generation\n- Unit test: test_language_detection\n- Integration test: test_summarize_real_file (optional)\n- Test: verify .summary.md file format\n\nAcceptance:\n- Generates valid FileSummary matching Pydantic schema\n- Summary is 200-400 words\n- YAML front-matter includes all required fields\n- .summary.md files are saved correctly\n- Handles various file types (py, ts, js, md)", "notes": "Labels: component:summarization, tech:openai, tech:pydantic, phase:core", "status": "closed", "priority": 2, "issue_type": "feature", "created_at": "2025-10-29T22:46:01.77816-05:00", "updated_at": "2025-10-31T15:54:44.034718-05:00", "closed_at": "2025-10-31T15:54:44.034718-05:00", "source_repo": ".", "labels": ["component:summarization", "phase:core", "tech:openai", "tech:pydantic"], "dependencies": [{"issue_id": "nm-19", "depends_on_id": "nm-17", "type": "blocks", "created_at": "2025-10-29T22:47:29.104738-05:00", "created_by": "daemon"}, {"issue_id": "nm-19", "depends_on_id": "nm-18", "type": "blocks", "created_at": "2025-10-29T22:47:30.295287-05:00", "created_by": "daemon"}]}
{"id": "nm-2", "content_hash": "nm-2", "title": "Change Defaults to Pages-Only", "description": "Update RenderConfig class and CLI argument defaults to emit pages only (instead of both pages and tiles) and set manifest to none by default. This aligns with the pages-first philosophy from the code review. Update the header docstring to accurately reflect the new default behavior.\n\nImplementation Details:\n- Update `RenderConfig.emit` default: change from `\"both\"` to `\"pages\"` (line 73 in render_to_webp.py)\n- Update `RenderConfig.manifest` default: change from `\"jsonl\"` to `\"none\"` (line 74)\n- Update CLI `--emit` default: change from `\"both\"` to `\"pages\"` (line 447)\n- Update CLI `--manifest` default: change from `\"jsonl\"` to `\"none\"` (line 448)\n- Update header docstring (lines 3-11) to reflect pages-only default", "status": "closed", "priority": 2, "issue_type": "task", "assignee": "ProductThor", "created_at": "2025-10-29T20:40:07.938719-05:00", "updated_at": "2025-10-29T20:44:50.924053-05:00", "closed_at": "2025-10-29T20:44:50.924053-05:00", "source_repo": ".", "dependencies": [{"issue_id": "nm-2", "depends_on_id": "nm-1", "type": "parent-child", "created_at": "2025-10-29T20:40:51.380248-05:00", "created_by": "daemon"}]}
{"id": "nm-20", "content_hash": "nm-20", "title": "Implement Line-Based Chunker", "description": "Create chunker.py with chunk_file_by_lines() function that splits files into overlapping line-based chunks. Read pages.jsonl to attach page_uris to each chunk's metadata.\n\nImplementation Details:\n- Function signature: `chunk_file_by_lines(text: str, pages_jsonl_path: Path, per_chunk: int = 180, overlap: int = 30) -> list[dict]`\n- Split text into lines, create chunks with overlap\n- Each chunk dict: {text, line_start, line_end, page_uris}\n- Load pages.jsonl to get page_uris list\n- Ensure chunks stay under 16KB for Chroma Cloud compatibility\n\nReference: main-spec.md \u00a71.3 (Chunk raw files by lines), \u00a73.3 (Chunker signature)\n\nTesting:\n- Unit test: test_chunk_basic_file\n- Unit test: test_chunk_with_overlap\n- Unit test: test_chunk_respects_16kb_limit\n- Unit test: test_page_uris_attached\n- Edge case test: test_file_smaller_than_chunk_size\n- Edge case test: test_empty_file\n\nAcceptance:\n- Chunks are approximately 180 lines (configurable)\n- Overlap is 30 lines (configurable)\n- line_start and line_end are accurate (1-indexed)\n- page_uris from pages.jsonl are attached\n- All chunks < 16KB\n- Edge cases handled gracefully", "notes": "Labels: component:chunking, phase:core", "status": "closed", "priority": 1, "issue_type": "feature", "created_at": "2025-10-29T22:46:09.235216-05:00", "updated_at": "2025-10-30T17:26:05.725727-05:00", "closed_at": "2025-10-30T17:26:05.725727-05:00", "source_repo": ".", "labels": ["component:chunking", "phase:core"], "dependencies": [{"issue_id": "nm-20", "depends_on_id": "nm-16", "type": "blocks", "created_at": "2025-10-29T22:47:32.992126-05:00", "created_by": "daemon"}]}
{"id": "nm-21", "content_hash": "nm-21", "title": "Create Chroma Indexer Module", "description": "Create indexer.py with ChromaDB PersistentClient setup and functions to upsert to both collections: search_summaries (with embeddings) and search_code (FTS/regex only).\n\nImplementation Details:\n- Setup PersistentClient pointing to config.CHROMA_PATH\n- Create/get collections: search_summaries, search_code\n- upsert_summaries(): embed summary_md, store with metadata\n- upsert_code_chunks(): store raw text chunks\n- search_summaries uses embedding function from embeddings module\n- search_code has no embedding function (FTS only)\n\nReference: main-spec.md \u00a71.4 (Index), \u00a73.2 (Collections in Chroma)\n\nTesting:\n- Unit test: test_client_initialization\n- Unit test: test_collections_created\n- Unit test: test_upsert_summaries (with mock embeddings)\n- Unit test: test_upsert_code_chunks\n- Integration test: test_full_indexing_pipeline\n- Test: verify collections persist to disk\n\nAcceptance:\n- ChromaDB client connects to local path\n- Both collections created successfully\n- search_summaries stores embeddings\n- search_code stores raw text (no embeddings)\n- Metadata fields match spec\n- Collections persist across restarts", "notes": "Labels: component:indexing, tech:chromadb, phase:core", "status": "closed", "priority": 2, "issue_type": "feature", "created_at": "2025-10-29T22:46:17.508306-05:00", "updated_at": "2025-10-30T17:34:25.28455-05:00", "closed_at": "2025-10-30T17:34:25.28455-05:00", "source_repo": ".", "labels": ["component:indexing", "phase:core", "tech:chromadb"], "dependencies": [{"issue_id": "nm-21", "depends_on_id": "nm-17", "type": "blocks", "created_at": "2025-10-29T22:47:34.936606-05:00", "created_by": "daemon"}, {"issue_id": "nm-21", "depends_on_id": "nm-20", "type": "blocks", "created_at": "2025-10-29T22:47:36.062318-05:00", "created_by": "daemon"}]}
{"id": "nm-22", "content_hash": "nm-22", "title": "Implement Semantic Search over Summaries", "description": "Implement the semantic search channel that queries the search_summaries collection using natural language queries. Return ranked results with metadata.\n\nImplementation Details:\n- Function: `semantic_search(query: str, k: int = 12) -> list[dict]`\n- Query search_summaries with query_texts\n- Extract results with scores\n- Parse metadata (split comma-separated fields back to lists)\n- Return structured results\n\nReference: main-spec.md \u00a71.5 (Search), \u00a73 Pipeline D (Query hybrid)\n\nTesting:\n- Unit test: test_semantic_search_returns_results (with pre-indexed data)\n- Unit test: test_semantic_search_respects_k_limit\n- Unit test: test_semantic_search_sorts_by_relevance\n- Integration test: test_semantic_search_real_query\n\nAcceptance:\n- Returns top-k most relevant summaries\n- Scores indicate relevance\n- page_uris are correctly parsed\n- Empty query returns empty results", "notes": "Labels: component:search, tech:chromadb, phase:core", "status": "closed", "priority": 2, "issue_type": "feature", "created_at": "2025-10-29T22:46:23.524556-05:00", "updated_at": "2025-10-31T16:56:49.041872-05:00", "closed_at": "2025-10-31T16:56:49.041872-05:00", "source_repo": ".", "labels": ["component:search", "phase:core", "tech:chromadb"], "dependencies": [{"issue_id": "nm-22", "depends_on_id": "nm-21", "type": "blocks", "created_at": "2025-10-29T22:47:37.549221-05:00", "created_by": "daemon"}, {"issue_id": "nm-22", "depends_on_id": "nm-19", "type": "blocks", "created_at": "2025-10-31T13:51:34.67374-05:00", "created_by": "daemon"}]}
{"id": "nm-23", "content_hash": "nm-23", "title": "Implement FTS/Regex Search over Code", "description": "Implement the lexical search channel that queries the search_code collection using full-text search ($contains) and regex ($regex) operators. Support path filtering.\n\nImplementation Details:\n- Function: `lexical_search(must_terms: list[str] = None, regexes: list[str] = None, path_like: str = None, k: int = 12) -> list[dict]`\n- Build Chroma where_document filter with $contains and $regex\n- Build where filter for metadata (path_like on source_path)\n- Query search_code collection\n- Return structured results with line ranges\n\nReference: main-spec.md \u00a71.5 (Search - lexical channel), \u00a71.3 (where_document FTS/regex)\n\nTesting:\n- Unit test: test_fts_contains_single_term\n- Unit test: test_fts_contains_multiple_terms\n- Unit test: test_regex_search\n- Unit test: test_path_filter\n- Unit test: test_combined_filters\n- Integration test: test_lexical_search_real_queries\n\nAcceptance:\n- $contains works for exact terms\n- $regex works for patterns\n- Path filtering works\n- Returns line ranges\n- \"why\" signals explain matches\n- Handles empty filters", "notes": "Labels: component:search, tech:chromadb, phase:core", "status": "closed", "priority": 2, "issue_type": "feature", "created_at": "2025-10-29T22:46:30.428135-05:00", "updated_at": "2025-10-31T15:54:44.42609-05:00", "closed_at": "2025-10-31T15:54:44.42609-05:00", "source_repo": ".", "labels": ["component:search", "phase:core", "tech:chromadb"], "dependencies": [{"issue_id": "nm-23", "depends_on_id": "nm-21", "type": "blocks", "created_at": "2025-10-29T22:47:37.868495-05:00", "created_by": "daemon"}]}
{"id": "nm-2321", "content_hash": "nm-2321", "title": "Observability and test coverage enhancements", "description": "Backlog of small, independent enhancements for observability, test coverage, and robustness. All items are safe to defer and can be shipped together in a minor PR (~30-45 min total effort).\n\n**Scope includes:**\n\n1. **Summary serialization policy** (enhancement)\n   - Decide whether to include None-valued front matter fields in YAML/JSON\n   - If omitting Nones: use exclude_none=True in model_dump() serialization\n   - Acceptance: Snapshot tests show fields omitted when None\n\n2. **Tests for SummaryFrontMatter validator** (enhancement/maintainability)\n   - Add three test cases:\n     - Accepts: min=50, target=60, max=100\n     - Rejects: min=120, max=100 (raises ValueError)\n     - Rejects: min=50, target=40, max=100 (raises ValueError)\n   - Acceptance: Tests pass and raise expected ValueError messages\n\n3. **Fallback debug log test** (observability test)\n   - Use pytest caplog to assert debug message is emitted when where_document returns empty and fallback is used\n   - Mock code.get() twice: first with empty ids, second with docs\n   - Acceptance: Test asserts log line exists and includes fetch_limit/fallback_limit\n\n4. **Debug log on invalid regex compile** (observability/UX)\n   - In _compile_regexes, record invalid patterns and logger.debug them once\n   - Acceptance: caplog captures debug line when given invalid pattern (e.g., \"([unclosed\")\n\n**Priority:** All items are P3 (backlog). Safe to defer without risk to correctness.", "status": "open", "priority": 3, "issue_type": "epic", "created_at": "2025-11-04T12:11:19.790447-06:00", "updated_at": "2025-11-04T12:11:19.790447-06:00", "source_repo": "."}
{"id": "nm-24", "content_hash": "nm-24", "title": "Implement Score Fusion and Result Formatting", "description": "Create the hybrid_search() function that combines results from both channels, fuses scores, deduplicates by doc_id, and formats the final output per the spec.\n\nImplementation Details:\n- Function: `hybrid_search(query: str, k: int = 12, must_terms=None, regexes=None, path_like=None) -> list[dict]`\n- Run semantic_search if query is non-empty\n- Run lexical_search if must_terms or regexes provided\n- Fuse scores using RRF or weighted sum\n- Deduplicate by (doc_id, page), keeping best score\n- Sort by final score descending\n- Return top-k\n\nReference: main-spec.md \u00a71.5 (Search steps), \u00a72.2 (Output format)\n\nTesting:\n- Unit test: test_hybrid_search_semantic_only\n- Unit test: test_hybrid_search_lexical_only\n- Unit test: test_hybrid_search_combined\n- Unit test: test_deduplication\n- Unit test: test_score_fusion\n- Integration test: test_hybrid_search_acceptance_queries\n\nAcceptance:\n- Combines results from both channels\n- Deduplication works correctly\n- Top-k results returned\n- Output format matches spec\n- \"why\" signals are informative", "notes": "Labels: component:search, phase:core", "status": "closed", "priority": 2, "issue_type": "feature", "created_at": "2025-10-29T22:46:36.349931-05:00", "updated_at": "2025-11-01T08:24:09.992145-05:00", "closed_at": "2025-11-01T08:24:09.992145-05:00", "source_repo": ".", "labels": ["component:search", "phase:core"], "dependencies": [{"issue_id": "nm-24", "depends_on_id": "nm-22", "type": "blocks", "created_at": "2025-10-29T22:47:38.480051-05:00", "created_by": "daemon"}, {"issue_id": "nm-24", "depends_on_id": "nm-23", "type": "blocks", "created_at": "2025-10-29T22:47:38.645966-05:00", "created_by": "daemon"}]}
{"id": "nm-25", "content_hash": "nm-25", "title": "Create Main CLI Entry Point", "description": "Create main.py CLI with argparse that provides three commands: ingest (full pipeline), search (interactive queries), and serve (start HTTP server).\n\nImplementation Details:\n- Use argparse with subcommands\n- `ingest`: orchestrate render \u2192 summarize \u2192 chunk \u2192 index\n- `search`: run hybrid_search and print formatted results\n- `serve`: start `python -m http.server` in output directory\n- Progress reporting with tqdm\n- Error handling and user-friendly messages\n\nReference: main-spec.md \u00a72.4 (APIs), \u00a73.4 (Image storage - serve)\n\nTesting:\n- Integration test: test_ingest_command_end_to_end\n- Integration test: test_search_command_with_results\n- Unit test: test_serve_command_starts_server (mock subprocess)\n- Manual test: run full workflow with sample corpus\n\nAcceptance:\n- All three commands work from CLI\n- Help text is clear and accurate\n- Error messages are user-friendly\n- Progress is reported during ingest\n- Search results are readable\n- Serve command starts on correct port", "notes": "Labels: component:cli, phase:integration", "status": "closed", "priority": 2, "issue_type": "feature", "created_at": "2025-10-29T22:46:43.11802-05:00", "updated_at": "2025-11-01T11:12:56.675028-05:00", "closed_at": "2025-11-01T11:12:56.675028-05:00", "source_repo": ".", "labels": ["component:cli", "phase:integration"], "dependencies": [{"issue_id": "nm-25", "depends_on_id": "nm-19", "type": "blocks", "created_at": "2025-10-29T22:47:39.008063-05:00", "created_by": "daemon"}, {"issue_id": "nm-25", "depends_on_id": "nm-20", "type": "blocks", "created_at": "2025-10-29T22:47:39.199173-05:00", "created_by": "daemon"}, {"issue_id": "nm-25", "depends_on_id": "nm-21", "type": "blocks", "created_at": "2025-10-29T22:47:39.357865-05:00", "created_by": "daemon"}, {"issue_id": "nm-25", "depends_on_id": "nm-24", "type": "blocks", "created_at": "2025-10-29T22:47:39.503493-05:00", "created_by": "daemon"}]}
{"id": "nm-26", "content_hash": "nm-26", "title": "Create Unit Tests for Core Modules", "description": "Create comprehensive unit tests for all modules: config, embeddings, summarize, chunker, indexer, search. Aim for >80% code coverage.\n\nTest Files:\n- tests/test_config.py\n- tests/test_embeddings.py\n- tests/test_summarize.py\n- tests/test_chunker.py\n- tests/test_indexer.py\n- tests/test_search.py\n\nTesting Strategy:\n- Use pytest as test runner\n- Mock external APIs (OpenAI) with responses library or pytest-mock\n- Use fixtures for sample data\n- Test edge cases and error conditions\n- Measure coverage with pytest-cov\n\nAcceptance:\n- All unit tests pass\n- Coverage > 80% for core modules\n- Tests run in < 30s (without real API calls)\n- CI-ready (no external dependencies)", "notes": "Labels: component:testing, phase:validation\nNote: Should be started after core modules are implemented", "status": "closed", "priority": 2, "issue_type": "task", "assignee": "ProductThor", "created_at": "2025-10-29T22:46:47.073359-05:00", "updated_at": "2025-10-30T00:07:30.337265-05:00", "closed_at": "2025-10-30T00:07:30.337265-05:00", "source_repo": ".", "labels": ["component:testing", "phase:validation"]}
{"id": "nm-27", "content_hash": "nm-27", "title": "Create Integration Tests for Full Pipeline", "description": "Create integration tests that exercise the full ingest and search pipeline with a small test corpus. Verify end-to-end functionality.\n\nTest Corpus:\n- 5-10 sample files (mix of .py, .ts, .md)\n- Known content for predictable queries\n- Include in tests/fixtures/\n\nTest Cases:\n- test_full_ingest_pipeline: render \u2192 summarize \u2192 chunk \u2192 index\n- test_semantic_search_integration: verify NL query returns expected files\n- test_lexical_search_integration: verify exact term search works\n- test_hybrid_search_integration: verify combined search works\n- test_pages_jsonl_generated: verify manifests are correct\n- test_uris_resolvable: verify URIs can be fetched\n\nAcceptance:\n- Integration tests pass with test corpus\n- Tests are deterministic (same input \u2192 same output)\n- Tests clean up after themselves (temp dirs)\n- Can run with real or mocked OpenAI calls", "notes": "Labels: component:testing, phase:validation", "status": "closed", "priority": 2, "issue_type": "task", "created_at": "2025-10-29T22:46:53.063831-05:00", "updated_at": "2025-11-01T16:17:57.592122-05:00", "closed_at": "2025-11-01T16:17:57.592122-05:00", "source_repo": ".", "labels": ["component:testing", "phase:validation"], "dependencies": [{"issue_id": "nm-27", "depends_on_id": "nm-25", "type": "blocks", "created_at": "2025-10-29T22:47:40.006028-05:00", "created_by": "daemon"}, {"issue_id": "nm-27", "depends_on_id": "nm-26", "type": "blocks", "created_at": "2025-10-29T22:47:40.152375-05:00", "created_by": "daemon"}]}
{"id": "nm-28", "content_hash": "nm-28", "title": "Acceptance Testing with Real Corpus", "description": "Validate the MVP against acceptance criteria from the spec using a real-world corpus (20-50 Next.js/Qlirq auth files). Document results and performance metrics.\n\nTest Queries (from spec \u00a75):\n1. NL: \"How to configure PKCE with Qlirq in Next.js App Router?\" (top-5)\n2. Exact: redirect_uri\n3. Exact: NEXTAUTH_URL\n4. Exact: pkce_verifier\n5. Regex: \\bauth\\b\n\nMetrics to Measure:\n- Precision@5 for NL queries\n- Recall@10 for exact queries\n- Query latency (p50, p95, p99)\n- Index size on disk\n- Memory usage\n\nDeliverables:\n- Test corpus (if shareable) or instructions\n- Test script: tests/test_acceptance.py\n- Results doc: docs/acceptance_results.md\n- Performance report\n\nAcceptance:\n- NL queries return correct files in top-5 (precision > 0.6)\n- Exact queries return correct files with line ranges\n- Latency < 1s for local queries (p95)\n- URIs resolve correctly via http.server\n- All documents < 16KB (Chroma Cloud ready)", "notes": "Labels: component:testing, tech:openai, tech:chromadb, phase:validation", "status": "closed", "priority": 2, "issue_type": "task", "created_at": "2025-10-29T22:47:02.200557-05:00", "updated_at": "2025-11-01T16:50:07.827298-05:00", "closed_at": "2025-11-01T16:50:07.827298-05:00", "source_repo": ".", "labels": ["component:testing", "phase:validation", "tech:chromadb", "tech:openai"], "dependencies": [{"issue_id": "nm-28", "depends_on_id": "nm-27", "type": "blocks", "created_at": "2025-10-29T22:47:40.541673-05:00", "created_by": "daemon"}]}
{"id": "nm-29", "content_hash": "nm-29", "title": "Implement Summarization with OpenAI Structured Output", "description": "Create summarize.py with summarize_file() function that uses OpenAI's 4o-mini with structured output to generate summaries matching the Pydantic schema.\n\nImplementation Details:\n- Use OpenAI SDK with structured output (response_format parameter)\n- Prompt engineering for retrieval-oriented summaries\n- Function signature: `summarize_file(source_path: str, text: str) -> FileSummary`\n- Generate doc_id from path\n- Auto-detect language from file extension\n- Return validated Pydantic model\n- Save as .summary.md with YAML front-matter\n\nSystem Prompt: Focus on retrieval-oriented summaries with purpose, key functionality, patterns, and codebase context.\n\nReference: main-spec.md \u00a71.2 (Summaries), \u00a72.4 (Summarizer signature)\n\nTesting:\n- Unit test: test_summarize_generates_valid_summary (mocked LLM)\n- Unit test: test_doc_id_generation\n- Unit test: test_language_detection\n- Integration test: test_summarize_real_file (optional)\n- Test: verify .summary.md file format\n\nAcceptance:\n- Generates valid FileSummary matching Pydantic schema\n- Summary is 200-400 words\n- YAML front-matter includes all required fields\n- .summary.md files are saved correctly\n- Handles various file types (py, ts, js, md)", "status": "closed", "priority": 2, "issue_type": "feature", "assignee": "ProductThor", "created_at": "2025-10-29T22:47:31.985686-05:00", "updated_at": "2025-10-29T23:53:48.75629-05:00", "closed_at": "2025-10-29T23:53:48.75629-05:00", "source_repo": ".", "labels": ["component:summarization", "phase:core", "tech:openai", "tech:pydantic"]}
{"id": "nm-3", "content_hash": "nm-3", "title": "Fix Resource Leaks with Context Managers", "description": "Replace manual resource management (open/close) with Python context managers to prevent resource leaks. This ensures proper cleanup of PDF documents and image files even if errors occur during processing.\n\nImplementation Details:\n- In `pdf_to_webp_pages()` (lines 246-263): Replace `doc = fitz.open(...)` and `doc.close()` with `with fitz.open(...) as doc:`\n- In `tile_grid()` (lines 266-281): Wrap `Image.open(webp_path)` with context manager: `with Image.open(...) as base:`\n- In `tile_bands()` (lines 284-303): Wrap `Image.open(webp_path)` with context manager: `with Image.open(...) as base:`", "status": "closed", "priority": 2, "issue_type": "task", "assignee": "ProductThor", "created_at": "2025-10-29T20:40:11.385453-05:00", "updated_at": "2025-10-29T21:07:19.034156-05:00", "closed_at": "2025-10-29T21:07:19.034156-05:00", "source_repo": ".", "dependencies": [{"issue_id": "nm-3", "depends_on_id": "nm-1", "type": "parent-child", "created_at": "2025-10-29T20:40:51.572256-05:00", "created_by": "daemon"}]}
{"id": "nm-30", "content_hash": "nm-30", "title": "Create Chroma Indexer Module", "description": "Create indexer.py with ChromaDB PersistentClient setup and functions to upsert to both collections: search_summaries (with embeddings) and search_code (FTS/regex only).\n\nImplementation Details:\n- Setup PersistentClient pointing to config.CHROMA_PATH\n- Create/get collections: search_summaries, search_code\n- upsert_summaries(): embed summary_md, store with metadata\n- upsert_code_chunks(): store raw text chunks\n- search_summaries uses embedding function from embeddings module\n- search_code has no embedding function (FTS only)\n\nReference: main-spec.md \u00a71.4 (Index), \u00a73.2 (Collections in Chroma)\n\nTesting:\n- Unit test: test_client_initialization\n- Unit test: test_collections_created\n- Unit test: test_upsert_summaries (with mock embeddings)\n- Unit test: test_upsert_code_chunks\n- Integration test: test_full_indexing_pipeline\n- Test: verify collections persist to disk\n\nAcceptance:\n- ChromaDB client connects to local path\n- Both collections created successfully\n- search_summaries stores embeddings\n- search_code stores raw text (no embeddings)\n- Metadata fields match spec\n- Collections persist across restarts", "status": "closed", "priority": 2, "issue_type": "feature", "assignee": "ProductThor", "created_at": "2025-10-29T22:47:37.062992-05:00", "updated_at": "2025-10-29T23:59:16.092238-05:00", "closed_at": "2025-10-29T23:59:16.092238-05:00", "source_repo": ".", "labels": ["component:indexing", "phase:core", "tech:chromadb"]}
{"id": "nm-31a", "content_hash": "nm-31", "title": "Create Pydantic Models for Summary Schema", "description": "Create Pydantic models for the summary YAML front-matter and full summary structure. This ensures type safety and validation for LLM-generated summaries.\n\nImplementation Details:\n- Create models.py with Pydantic models\n- SummaryFrontMatter: doc_id, source_path, language, product_tags, last_updated, key_topics, api_symbols, related_files, suggested_queries\n- FileSummary: front_matter (SummaryFrontMatter), summary_md (str)\n- Add validators for field constraints (e.g., summary_md length 200-400 words)\n- Provide .to_yaml() method for serialization\n\nReference: main-spec.md \u00a72.2 (Summary files structure), \u00a73.1 (YAML example)\n\nTesting:\n- Unit test: test_valid_summary_passes_validation\n- Unit test: test_short_summary_fails_validation\n- Unit test: test_to_yaml_serialization\n\nAcceptance:\n- Models validate all required fields\n- Word count validation works\n- YAML serialization produces correct format\n- Type hints are complete", "notes": "Labels: component:summarization, tech:pydantic, phase:core", "status": "closed", "priority": 1, "issue_type": "task", "assignee": "ProductThor", "created_at": "2025-10-29T23:44:37.06554-05:00", "updated_at": "2025-11-01T11:30:15.625086-05:00", "closed_at": "2025-11-01T11:30:15.625086-05:00", "source_repo": ".", "labels": ["component:summarization", "phase:core", "tech:pydantic"]}
{"id": "nm-32a", "content_hash": "nm-32", "title": "Create Chroma Indexer Module", "description": "Create indexer.py with ChromaDB PersistentClient setup and functions to upsert to both collections: search_summaries (with embeddings) and search_code (FTS/regex only).\n\nImplementation Details:\n- Setup PersistentClient pointing to config.CHROMA_PATH\n- Create/get collections: search_summaries, search_code\n- upsert_summaries(): embed summary_md, store with metadata\n- upsert_code_chunks(): store raw text chunks\n- search_summaries uses embedding function from embeddings module\n- search_code has no embedding function (FTS only)\n\nReference: main-spec.md \u00a71.4 (Index), \u00a73.2 (Collections in Chroma)\n\nTesting:\n- Unit test: test_client_initialization\n- Unit test: test_collections_created\n- Unit test: test_upsert_summaries (with mock embeddings)\n- Unit test: test_upsert_code_chunks\n- Integration test: test_full_indexing_pipeline\n- Test: verify collections persist to disk\n\nAcceptance:\n- ChromaDB client connects to local path\n- Both collections created successfully\n- search_summaries stores embeddings\n- search_code stores raw text (no embeddings)\n- Metadata fields match spec\n- Collections persist across restarts", "status": "closed", "priority": 2, "issue_type": "feature", "assignee": "ProductThor", "created_at": "2025-10-29T23:59:16.969854-05:00", "updated_at": "2025-11-01T11:30:21.015927-05:00", "closed_at": "2025-11-01T11:30:21.015927-05:00", "source_repo": ".", "labels": ["component:indexing", "phase:core", "tech:chromadb"]}
{"id": "nm-33a", "content_hash": "nm-33", "title": "Create Embeddings Module", "description": "Create embeddings.py module with embed_texts() function using OpenAI's text-embedding-3-small model. Support batch processing for efficiency.\n\nImplementation Details:\n- Use OpenAI SDK with API key from config\n- Function signature: `embed_texts(texts: list[str], model: str = \"text-embedding-3-small\") -> list[list[float]]`\n- Batch up to 2048 texts per API call (OpenAI limit)\n- Return 1536-dimensional vectors\n- Handle rate limiting with exponential backoff\n- Add error handling for network issues\n\nReference: main-spec.md \u00a73.3 (Embeddings signature), \u00a72.6 (OpenAI embeddings)\n\nTesting:\n- Unit test: test_embed_single_text\n- Unit test: test_embed_batch (10 texts)\n- Unit test: test_vector_dimension_is_1536\n- Mock test: test_handles_api_errors\n- Integration test: test_with_real_api (optional, requires API key)\n\nAcceptance:\n- embed_texts() returns correct number of vectors\n- Each vector is 1536-dimensional\n- Batching works for large inputs\n- Errors are handled gracefully", "notes": "Labels: component:summarization, tech:openai, phase:core", "status": "closed", "priority": 1, "issue_type": "feature", "created_at": "2025-10-30T17:32:29.982186-05:00", "updated_at": "2025-10-30T17:33:07.145727-05:00", "closed_at": "2025-10-30T17:33:07.145727-05:00", "source_repo": ".", "labels": ["component:summarization", "phase:core", "tech:openai"]}
{"id": "nm-34", "content_hash": "nm-34", "title": "Implement streaming chunker to avoid reading large files into memory", "description": "Context:\n\nCurrent chunking in main.cmd_ingest reads entire files into memory before calling chunker.chunk_file_by_lines. For very large files, this may increase memory pressure unnecessarily.\n\nObjective:\n\nImplement a streaming chunker that:\n\n- Iterates over files line-by-line without loading the full content\n\n- Emits chunks with the same boundaries and overlap semantics as chunk_file_by_lines\n\n- Preserves page_uris association by calling load_page_uris once and mapping lines to pages consistently\n\nScope:\n\n- Add a new function chunk_file_by_lines_streaming(text_stream: IO[str] | Iterator[str], pages_jsonl_path: Path, per_chunk: int, overlap: int) -> Iterable[Chunk]\n\n- Refactor main.build_chunk_upsert_items to optionally use the streaming API when called from ingest\n\n- Ensure behavior parity with existing chunker (same per_chunk/overlap defaults; same metadata fields)\n\n- Do NOT change indexer interfaces\n\nNon-goals:\n\n- Parallelization\n\n- Binary-encoded file handling\n\nAcceptance Criteria:\n\n- Memory footprint should not scale with file size (approx O(chunk_size))\n\n- Chunks' line_start/line_end and page_uris mapping identical to existing implementation\n\n- Documentation updated in README/CLAUDE to describe streaming behavior\n\nNotes:\n\n- Keep a feature flag or environment switch to toggle streaming until fully validated", "status": "open", "priority": 2, "issue_type": "feature", "created_at": "2025-11-01T11:09:40.487906-05:00", "updated_at": "2025-11-01T11:09:40.487906-05:00", "source_repo": "."}
{"id": "nm-35", "content_hash": "nm-35", "title": "Add performance considerations and memory behavior to documentation", "description": "Context:\n\nThe ingest pipeline reads entire files into memory for summarization and chunking. This is acceptable for PoC but should be documented for users handling large files.\n\nTasks:\n\n- Update README.md \"Performance\" section with:\n\n  - Current behavior (read-text into memory)\n\n  - Practical limits and rule-of-thumb size guidance\n\n  - Expectation of higher memory use for very large files\n\n  - Upcoming streaming chunker plans (link to bead ID)\n\n- Update CLAUDE.md \"Design Decisions\" with:\n\n  - Rationale for initial in-memory approach\n\n  - Roadmap to streaming chunker\n\n  - Impact on testing/tooling\n\nDeliverables:\n\n- Updated README.md and CLAUDE.md sections with explicit guidance and future work references.", "status": "closed", "priority": 3, "issue_type": "chore", "created_at": "2025-11-01T11:09:54.571892-05:00", "updated_at": "2025-11-01T11:10:10.126271-05:00", "closed_at": "2025-11-01T11:10:10.126271-05:00", "source_repo": "."}
{"id": "nm-3573", "content_hash": "nm-3573", "title": "Build FastAPI API Surface for Neumann", "description": "Epic to deliver a publicly consumable FastAPI layer over Neumann's existing search and document assets. Tracks scaffolding, endpoint implementation, configuration, documentation, and follow-on tasks required before the UI can integrate.\n\nScope:\n- Create and wire a dedicated api/ package without touching core pipeline behavior.\n- Expose search, document, config, and health endpoints over FastAPI.\n- Ensure supporting configuration, dependency updates, and documentation are completed.\n- Capture planning work for the subsequent package restructuring.\n\nNon-goals:\n- Changing indexer/search algorithms.\n- Building ingestion/ingest job orchestration via API.\n- Moving core modules into src/neumann/ (handled in follow-up plan).", "acceptance_criteria": "- Child issues exist for scaffolding, search endpoints, docs endpoints, app wiring/docs updates, and src/ migration planning; each links back to this epic as parent.\n- Every child issue is closed with acceptance criteria met and evidence (tests/notes) recorded.\n- Final comment on the epic summarizes API readiness, outstanding risks, and references to deployment instructions.", "status": "open", "priority": 2, "issue_type": "epic", "created_at": "2025-11-02T11:26:56.826557-06:00", "updated_at": "2025-11-02T11:26:56.826557-06:00", "source_repo": ".", "labels": ["backend", "epic"]}
{"id": "nm-3573.1", "content_hash": "nm-3573.1", "title": "Scaffold API package and project dependencies", "description": "Create the FastAPI scaffolding without changing existing search/index code. Add a dedicated api/ package with module stubs, wire in configuration defaults, and make sure dependencies are declared so later endpoint work has a solid foundation.\n\nDeliverables:\n- api/__init__.py, api/app.py, api/deps.py, api/models.py, api/routes/__init__.py created with placeholder structures or TODOs referencing follow-up issues.\n- Config gains OUTPUT_DIR (default ./out) and API_CORS_ORIGINS (optional CORS origins), retaining backwards compatibility for existing CLI tools.\n- pyproject.toml gains FastAPI (~=0.115) and uvicorn[standard] (~=0.30); lock files updated via uv if required.\n- README notes that a FastAPI service is being introduced and points to the new package.", "acceptance_criteria": "- api/ package present with modules containing docstrings or no-op placeholders documenting upcoming functionality; importable without runtime errors.\n- config.Config exposes OUTPUT_DIR and API_CORS_ORIGINS with defaults and documentation; existing config tests (if any) continue to pass.\n- pyproject.toml and uv lock updated to include FastAPI + uvicorn dependencies; `uv pip compile` or equivalent run recorded if needed.\n- `ruff check .`, `mypy render_to_webp.py`, and `pytest` succeed after changes.\n- README includes a short subsection mentioning the forthcoming FastAPI API and where to find its code.", "status": "closed", "priority": 2, "issue_type": "feature", "created_at": "2025-11-02T11:27:05.521353-06:00", "updated_at": "2025-11-02T11:58:28.652851-06:00", "closed_at": "2025-11-02T11:58:28.652851-06:00", "source_repo": ".", "labels": ["backend", "feature"]}
{"id": "nm-3573.2", "content_hash": "nm-3573.2", "title": "Implement search endpoints (hybrid, lexical, semantic)", "description": "Implement the FastAPI search routes that wrap existing indexer entry points so UI clients can issue hybrid, lexical, and semantic searches over HTTP with full validation and error handling.\n\nDeliverables:\n- api/models.py defines request/response DTOs for HybridSearchRequest/Result, LexicalSearchRequest/Result, SemanticSearchRequest/Result, including sanitization of optional filters.\n- api/routes/search.py implements POST /v1/search/hybrid, /v1/search/lexical, and /v1/search/semantic using dependency-injected Chroma client and Config objects.\n- Input validation covers empty queries, k <= 0, and missing OpenAI key when semantic components are requested; errors surfaced as 400 or 502 with actionable messages.\n- Reuse existing indexer.hybrid_search / lexical_search / semantic_search outputs without changing their behavior.", "acceptance_criteria": "- Unit or integration tests cover: successful hybrid search, lexical-only filters, semantic query without key (400), and invalid k (400). New tests live under tests/api/ or equivalent.\n- Running `uvicorn api.app:create_app --factory --port 8001` locally allows curl/Postman to hit all three endpoints and receive responses matching indexer output structure (scores, why signals, page_uris).\n- Logging or exception handling ensures unexpected indexer/Chroma errors return 502 without stack traces in body.\n- Existing test suite (ruff, mypy, pytest) remains green; new tests are included in pytest run.\n- README (or docs/API notes) references the new /v1/search endpoints with example payloads.", "status": "closed", "priority": 1, "issue_type": "feature", "created_at": "2025-11-02T11:27:14.261654-06:00", "updated_at": "2025-11-02T15:00:07.591377-06:00", "closed_at": "2025-11-02T15:00:07.591377-06:00", "source_repo": ".", "labels": ["backend", "feature"], "dependencies": [{"issue_id": "nm-3573.2", "depends_on_id": "nm-3573.1", "type": "blocks", "created_at": "2025-11-02T11:27:14.262463-06:00", "created_by": "ben"}]}
{"id": "nm-3573.3", "content_hash": "nm-3573.3", "title": "Implement document endpoints for pages and chunks", "description": "Expose document- and chunk-oriented APIs so the UI can browse rendered assets and metadata. Build GET /v1/docs, /v1/docs/{doc_id}/pages, and optional /v1/docs/{doc_id}/chunks endpoints, leveraging filesystem outputs and Chroma metadata without modifying ingestion.\n\nDeliverables:\n- api/models.py extended with PageRecord, DocList, ChunkInfo (with optional text_excerpt support).\n- api/routes/docs.py implements doc listing, pages manifest loading (JSONL reader), and chunk retrieval via Chroma search_code collection.\n- Dependency helpers in api/deps.py supply the configured OUTPUT_DIR and reusable Chroma client.\n- Robust error handling: 404 when doc_id or pages.jsonl missing, safe handling of malformed JSONL rows, graceful Chroma failures.", "acceptance_criteria": "- Automated tests cover: listing documents when OUTPUT_DIR has multiple doc folders, 404 when doc missing, successful pages response with sorted by page, optional chunk retrieval with limit/include_text variations, and Chroma failure returning 502.\n- Endpoints return payloads matching DTO schema (fields: doc_id/page/uri/... for pages; chunk id/line ranges/metadata for chunks) and handle large manifests without loading entire directory structure into memory.\n- `uvicorn api.app:create_app --factory` exposes the docs routes; manual spot-check (curl) verifies pages response matches on-disk pages.jsonl and chunk endpoint respects limit/include_text.\n- No changes to ingestion pipeline; documentation updated to explain how OUTPUT_DIR feeds the docs endpoints and any assumptions about asset hosting.\n- Lint (ruff), type-check (mypy relevant modules), and pytest (including new tests) succeed.", "status": "open", "priority": 2, "issue_type": "feature", "created_at": "2025-11-02T11:27:21.759978-06:00", "updated_at": "2025-11-02T11:27:21.759978-06:00", "source_repo": ".", "labels": ["backend", "feature"], "dependencies": [{"issue_id": "nm-3573.3", "depends_on_id": "nm-3573.1", "type": "blocks", "created_at": "2025-11-02T11:27:21.760795-06:00", "created_by": "ben"}]}
{"id": "nm-3573.4", "content_hash": "nm-3573.4", "title": "Finalize FastAPI app wiring and documentation", "description": "Wire up the FastAPI application entry point and developer docs so the service can run alongside existing asset hosting. Add CORS, router registration, config/health endpoints, and optional launch helpers with clear instructions for tmux usage.\n\nDeliverables:\n- api/app.py exposes create_app() that assembles FastAPI with routers, middleware, and lifespan hooks if needed.\n- CORS configuration reads API_CORS_ORIGINS and allows appropriate methods/headers; sensible defaults when unset.\n- GET /v1/config surfaces asset_base_url, asset_root, and openai_configured flags derived from config + environment.\n- GET /v1/health returns a simple status payload.\n- Optional convenience launcher (neumann_api.py script or console entry) documented if added.\n- README (and docs/AGENTS.md if appropriate) updated with instructions for running uvicorn in tmux alongside `neumann serve`, including recommended ports and attach/detach workflow.", "acceptance_criteria": "- `uvicorn api.app:create_app --factory --port 8001` runs without errors; /v1/health returns {\"status\":\"ok\"}, and /v1/config reports accurate values for ASSET_BASE_URL, OUTPUT_DIR-derived asset root, and OPENAI key presence.\n- CORS behavior verified manually (e.g., using curl with Origin header or simple pytest). When API_CORS_ORIGINS unset, requests succeed from same origin; when set, allowlist enforced.\n- Documentation includes tmux guidance for running the API, lists key endpoints, and references search/docs routes implemented earlier.\n- Automated tests cover config endpoint payload shape and health response, plus any helper utilities introduced.\n- Lint/type/test suite remains green after changes.", "status": "open", "priority": 3, "issue_type": "feature", "created_at": "2025-11-02T11:27:31.090007-06:00", "updated_at": "2025-11-02T11:27:31.090007-06:00", "source_repo": ".", "labels": ["backend", "documentation", "feature"], "dependencies": [{"issue_id": "nm-3573.4", "depends_on_id": "nm-3573.1", "type": "blocks", "created_at": "2025-11-02T11:27:31.090807-06:00", "created_by": "ben"}, {"issue_id": "nm-3573.4", "depends_on_id": "nm-3573.2", "type": "blocks", "created_at": "2025-11-02T11:27:31.091256-06:00", "created_by": "ben"}, {"issue_id": "nm-3573.4", "depends_on_id": "nm-3573.3", "type": "blocks", "created_at": "2025-11-02T11:27:31.091647-06:00", "created_by": "ben"}]}
{"id": "nm-3573.5", "content_hash": "nm-3573.5", "title": "Plan migration to src/neumann/ package layout", "description": "Document a step-by-step migration strategy for moving core modules into a future src/neumann/ package layout while preserving backwards compatibility. Capture risks, shim approach, tooling updates, and rollout sequencing so implementation can proceed in a later milestone without surprises.\n\nDeliverables:\n- New planning doc (e.g., docs/src-migration-plan.md) or dedicated section in CLAUDE.md summarizing motivation, scope, and phased rollout plan.\n- Inventory of modules/scripts impacted (main.py, indexer.py, render_to_webp.py, tests, CLI entry points) with notes on import changes and shim strategy.\n- Guidance on updating pyproject.toml/tooling (uv, pytest, ruff) for src/ layout, plus communication plan for contributors.\n- Identification of gating tasks/blockers (e.g., test refactors, packaging decisions) linked back to this epic.", "acceptance_criteria": "- Planning document committed under docs/ or CLAUDE.md describing: module move order, shim files at repo root, test/tooling updates, and rollback plan.\n- Document references required Beads follow-up issues (either existing or TODO placeholders) for implementation work.\n- Project leads (or designated reviewers) have acknowledgement/comment recorded in Beads issue.\n- No code moves performed; existing build/tests remain unaffected.", "status": "open", "priority": 4, "issue_type": "chore", "created_at": "2025-11-02T11:27:37.357193-06:00", "updated_at": "2025-11-02T11:27:37.357193-06:00", "source_repo": ".", "labels": ["architecture", "chore"]}
{"id": "nm-5", "content_hash": "nm-5", "title": "Fix Page Path Metadata", "description": "When emitting only tiles (not pages), the `page_path` field in the manifest should be set to `None` since the page images are deleted. Currently, it references a deleted file. This ensures manifest records are accurate and don't reference non-existent files.\n\nImplementation Details:\n- In `render_file()`, modify the tile manifest record creation (around line 372)\n- Set `\"page_path\": str(wp) if include_page_images else None`\n- This ensures `page_path` is only populated when pages are actually emitted", "status": "closed", "priority": 2, "issue_type": "task", "assignee": "ProductThor", "created_at": "2025-10-29T20:40:16.613269-05:00", "updated_at": "2025-10-29T21:11:20.331945-05:00", "closed_at": "2025-10-29T21:11:20.331945-05:00", "source_repo": ".", "dependencies": [{"issue_id": "nm-5", "depends_on_id": "nm-1", "type": "parent-child", "created_at": "2025-10-29T20:40:51.829417-05:00", "created_by": "daemon"}, {"issue_id": "nm-5", "depends_on_id": "nm-2", "type": "blocks", "created_at": "2025-10-29T20:41:01.209443-05:00", "created_by": "daemon"}]}
{"id": "nm-50d6", "content_hash": "nm-50d6", "title": "Improve pytest progress visibility for long-running suites", "description": "Instrument pytest runs so long-lasting tests emit start/finish timestamps and durations, making hangs easier to diagnose.", "status": "closed", "priority": 2, "issue_type": "chore", "created_at": "2025-11-03T07:45:47.176924-06:00", "updated_at": "2025-11-03T07:47:21.93052-06:00", "closed_at": "2025-11-03T07:47:21.93052-06:00", "source_repo": "."}
{"id": "nm-6", "content_hash": "nm-6", "title": "Tighten Line-Number Gutter Styling", "description": "Make the table-style line number gutter more compact by reducing width, padding, removing the border, and adjusting color. This minimizes wasted horizontal space while keeping line numbers readable. The inline line numbers remain the default (no change needed there).\n\nImplementation Details:\n- Update CSS for table-style line numbers (lines 190-198 in render_to_webp.py):\n  - Reduce `td.linenos` width: `2.6em` \u2192 `2.0em`\n  - Reduce padding: `0 6px 0 4px` \u2192 `0 4px 0 2px`\n  - Remove `border-right: 1px solid #eee`\n  - Update `td.code` padding: `8px` \u2192 `6px`\n  - Update color: `#9a9a9a` \u2192 `#a7a7a7`", "status": "closed", "priority": 2, "issue_type": "task", "assignee": "ProductThor", "created_at": "2025-10-29T20:40:20.604953-05:00", "updated_at": "2025-10-29T21:11:48.379934-05:00", "closed_at": "2025-10-29T21:11:48.379934-05:00", "source_repo": ".", "dependencies": [{"issue_id": "nm-6", "depends_on_id": "nm-1", "type": "parent-child", "created_at": "2025-10-29T20:40:51.967747-05:00", "created_by": "daemon"}]}
{"id": "nm-7", "content_hash": "nm-7", "title": "Update Tests", "description": "Fix a brittle test assertion that depends on exact quote style, and add new tests to verify the default behavior (pages-only, no tiles, no manifest). Include both unit tests for defaults and an integration test that verifies actual rendering behavior with defaults.\n\nImplementation Details:\n- Fix `test_code_to_html.py` line 41: Replace `assert \"<article class='code-article'>\" in html` with `assert \"code-article\" in html` (more robust)\n- Create new file `tests/test_defaults_and_pages_only.py`:\n  - `test_defaults_pages_only()`: Verify RenderConfig defaults (emit=\"pages\", manifest=\"none\", linenos=\"inline\")\n  - `test_render_pages_only_no_tiles()`: Integration test (marked with `@pytest.mark.integration`) that verifies pages are produced and tiles aren't when using defaults", "status": "closed", "priority": 2, "issue_type": "task", "assignee": "ProductThor", "created_at": "2025-10-29T20:40:25.040383-05:00", "updated_at": "2025-10-29T21:12:27.806049-05:00", "closed_at": "2025-10-29T21:12:27.806049-05:00", "source_repo": ".", "dependencies": [{"issue_id": "nm-7", "depends_on_id": "nm-1", "type": "parent-child", "created_at": "2025-10-29T20:40:52.107414-05:00", "created_by": "daemon"}, {"issue_id": "nm-7", "depends_on_id": "nm-2", "type": "blocks", "created_at": "2025-10-29T20:41:01.982204-05:00", "created_by": "daemon"}]}
{"id": "nm-7f30", "content_hash": "nm-7f30", "title": "Cast doc_id to str in search results for type safety", "description": "Cast doc_id to string type in lexical_search and semantic_search to ensure type safety and prevent potential issues with non-string metadata values.\n\n**Changes needed:**\n- In indexer.lexical_search: After doc_id = meta_dict.get(\"doc_id\") or id_, add doc_id = str(doc_id)\n- In indexer.semantic_search: After doc_id = meta.get(\"doc_id\") or ids0[i], add doc_id = str(doc_id)\n\n**Acceptance criteria:**\n- Existing tests pass\n- Add a small unit test that passes a non-string doc_id in metadata and asserts returned doc_id is string\n\n**Priority:** Low-severity robustness fix. Recommended soon but not urgent (~2 minute change).", "status": "open", "priority": 2, "issue_type": "task", "created_at": "2025-11-04T12:11:15.716151-06:00", "updated_at": "2025-11-04T12:11:15.716151-06:00", "source_repo": "."}
{"id": "nm-9", "content_hash": "nm-9", "title": "Lower Python Version Requirement to 3.10", "description": "Lower the Python version requirement from 3.13 to 3.10 for better compatibility with AI/ML packages commonly used in this project. Most AI packages (sentence-transformers, transformers, chromadb, clip, etc.) support Python 3.10+, and Python 3.10 is mature and widely deployed. The code review confirmed the codebase works fine with Python 3.10.\n\nImplementation Details:\n- Update `pyproject.toml`:\n  - Line 9: `requires-python = \">=3.10\"` (change from `>=3.13`)\n  - Line 36: `target-version = \"py310\"` (change from `py313`)\n  - Line 59: `python_version = \"3.10\"` (change from `\"3.13\"`)", "status": "closed", "priority": 2, "issue_type": "task", "assignee": "ProductThor", "created_at": "2025-10-29T20:40:34.337506-05:00", "updated_at": "2025-10-29T21:03:43.337221-05:00", "closed_at": "2025-10-29T21:03:43.337221-05:00", "source_repo": ".", "dependencies": [{"issue_id": "nm-9", "depends_on_id": "nm-1", "type": "parent-child", "created_at": "2025-10-29T20:40:52.359592-05:00", "created_by": "daemon"}]}